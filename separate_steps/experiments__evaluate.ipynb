{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaterinaKrupkina/decomposer_notebooks/blob/main/separate_steps/experiments__evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjuqO0o3oD1x"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import shlex\n",
        "import pandas as pd\n",
        "import time\n",
        "import sys\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkkUUNdfK_Gi"
      },
      "source": [
        "## Set up environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iufk8T9JLzVQ"
      },
      "source": [
        "## !!! After installation you'll be asked to reload the runtime. Please, REFUSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhWBFMKSpZsh"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZF6m8okeFCR"
      },
      "outputs": [],
      "source": [
        "REQUIREMENTS_PATH = \"/content/drive/MyDrive/HarmBench/decomposer_notebooks/requirements_full.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IFsB8TCkuXUL"
      },
      "outputs": [],
      "source": [
        "!pip install -r {REQUIREMENTS_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3WLPzYPmnwA"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/HarmBench"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set variables\n",
        "\n",
        "* WANDB_API_KEY\n",
        "* GITHUB_TOKEN\n",
        "* OPENAI_API_KEY\n",
        "* MISTRAL_API_KEY\n",
        "* CLAUDE_API_KEY\n",
        "* HF_HARMBENCH_CLASSIFIER_URL\n",
        "* HF_TOKEN"
      ],
      "metadata": {
        "id": "ofkfxTQtWDCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb setting\n",
        "os.environ[\"WANDB__EXECUTABLE\"] = \"/usr/local/bin/python\""
      ],
      "metadata": {
        "id": "pRN0vvzsWGJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyTyX4kbrg41"
      },
      "outputs": [],
      "source": [
        "def set_env_variable(variable_name):\n",
        "    \"\"\"\n",
        "    Sets an environment variable with the specified name. If the variable is already present\n",
        "    in the environment, it uses the existing value from `userdata`. If not, it prompts the user\n",
        "    to input the value and sets the environment variable with that input.\n",
        "\n",
        "    Args:\n",
        "        variable_name (str): The name of the environment variable to be set.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the variable_name is not present in `userdata` when it is expected.\n",
        "    \"\"\"\n",
        "    if userdata.get(variable_name):\n",
        "       os.environ[variable_name] = userdata.get(variable_name)\n",
        "    else:\n",
        "       os.environ[variable_name] = input(f\"Enter the value for {variable_name}: \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_env_variable(\"OPENAI_API_KEY\")\n",
        "set_env_variable(\"WANDB_API_KEY\")\n",
        "set_env_variable(\"GITHUB_TOKEN\")\n",
        "#set_env_variable(\"HF_HARMBENCH_CLASSIFIER_URL\")"
      ],
      "metadata": {
        "id": "PZppgH2CWIxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPb_sAs-G-iw"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3aVz3NiHeRD"
      },
      "outputs": [],
      "source": [
        "def run_evaluate_completions(experiment_name, behaviors_path):\n",
        "    \"\"\"\n",
        "    Run the evaluate_completions.py script as a subprocess.\n",
        "\n",
        "    This function executes the evaluate_completions.py script with the specified parameters,\n",
        "    captures its output in real-time, and prints it to the console.\n",
        "\n",
        "    Args:\n",
        "        behaviors_path (str): Path to the behaviors dataset file.\n",
        "        completions_path (str): Path to the completions file to be evaluated.\n",
        "        save_path (str): Path where the evaluation results will be saved.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        subprocess.CalledProcessError: If the subprocess returns a non-zero exit status.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up variables\n",
        "\n",
        "    completions_path = f'/content/drive/MyDrive/HarmBench/results/completions/completions_{experiment_name}.json'\n",
        "    save_path = f'/content/drive/MyDrive/HarmBench/results/evaled_{experiment_name}.json'\n",
        "\n",
        "    # Execute the script as a subprocess\n",
        "    command = [\n",
        "        sys.executable,\n",
        "        \"-u\",\n",
        "        \"evaluate_completions.py\",\n",
        "        f\"--behaviors_path={behaviors_path}\",\n",
        "        f\"--completions_path={completions_path}\",\n",
        "        f\"--save_path={save_path}\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        process = subprocess.Popen(\n",
        "            command,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        for line in process.stdout:\n",
        "            print(line, end='')\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "def process_results(experiment_name, project=\"test_db\", entity=\"palisaderesearch\"):\n",
        "    \"\"\"\n",
        "    Process the results of the evaluation and log them to Weights & Biases.\n",
        "\n",
        "    This function reads the evaluation results from a JSON file, creates a DataFrame,\n",
        "    and logs the results to Weights & Biases for tracking and visualization.\n",
        "\n",
        "    Args:\n",
        "        file (str): Name of the JSON file containing the evaluation results (without .json extension).\n",
        "        decomposer (bool): Flag indicating whether a decomposer was used in the experiment.\n",
        "        config_file (str): Path to the JSON config file to append to the run's config.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Set up variables\n",
        "    config_file=f'/content/drive/MyDrive/HarmBench/config_{experiment_name}.json'\n",
        "    evaled_file = f'/content/drive/MyDrive/HarmBench/results/evaled_{experiment_name}.json'\n",
        "\n",
        "    # Load the JSON data\n",
        "    data_eval = pd.read_json(evaled_file)\n",
        "    data_summ = pd.read_json(f\"/content/drive/MyDrive/HarmBench/summary_{experiment_name}.json\")\n",
        "\n",
        "    # Convert JSON data to DataFrame\n",
        "    df_evaled = pd.DataFrame([{\"category\": key, **value[0]} for key, value in data_eval.items()])\n",
        "    df = df_evaled.merge(data_summ, on='test_case', how='inner')\n",
        "\n",
        "    # Add additional columns\n",
        "    df[\"target_model\"] = os.getenv(\"MODEL_TARGET\")\n",
        "    df[\"decomposer_model\"] = os.getenv(\"MODEL_DECOMPOSER\")\n",
        "    df['composer_model'] = os.getenv(\"MODEL_COMPOSER\")\n",
        "    df[\"surrogate_model\"] = os.getenv(\"MODEL_SURROGATE\")\n",
        "    df[\"ASR\"] = df[\"label\"].mean()\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['test_case'])\n",
        "\n",
        "    # Remove the 'generation_y' column\n",
        "    df.drop('generation_y', axis=1, inplace=True)\n",
        "\n",
        "    # Rename 'generation_x' to 'generation'\n",
        "    df.rename(columns={'generation_x': 'generation'}, inplace=True)\n",
        "\n",
        "    # Rename columns to start with 'tokens_' or 'price_'\n",
        "    rename_dict = {\n",
        "        'surrogate_tokens': 'tokens_surrogate',\n",
        "        'target_tokens': 'tokens_target',\n",
        "        'decomposer_tokens': 'tokens_decomposer',\n",
        "        'composer_tokens': 'tokens_composer',\n",
        "        'surrogate_price': 'price_surrogate',\n",
        "        'target_price': 'price_target',\n",
        "        'decomposer_price': 'price_decomposer',\n",
        "        'composer_price': 'price_composer'\n",
        "    }\n",
        "    df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(df.head())\n",
        "\n",
        "    # Initialize W&B run\n",
        "    run = wandb.init(\n",
        "        entity=entity,\n",
        "        project=project,\n",
        "        save_code=False,\n",
        "        name=f\"decomp_{experiment_name}\"\n",
        "    )\n",
        "\n",
        "    # Load the config file and update run config\n",
        "    with open(config_file, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "        run.config.update(config_data)\n",
        "\n",
        "    # Log the DataFrame as a W&B Table\n",
        "    run.log({\"harmbench\": wandb.Table(dataframe=df)})\n",
        "    run.summary[\"harmbench_mean\"] = df[\"label\"].mean()\n",
        "    run.summary[\"target_model\"] = os.getenv(\"MODEL_TARGET\")\n",
        "\n",
        "    # Finish the W&B run\n",
        "    run.finish()\n"
      ],
      "metadata": {
        "id": "8UIr9OvAVEct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(experiment_name, experiment_params, project='test_db'):\n",
        "    \"\"\"\n",
        "    Run a complete experiment pipeline including completion generation, evaluation, and result processing.\n",
        "\n",
        "    This function sets up the environment, runs the necessary scripts for generating completions\n",
        "    and evaluating them, and then processes and logs the results.\n",
        "\n",
        "    Args:\n",
        "        experiment_params (dict): A dictionary containing experiment parameters including model names.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up environment variables\n",
        "    for k, v in experiment_params.items():\n",
        "        os.environ[k] = str(v)\n",
        "\n",
        "    try:\n",
        "      run_evaluate_completions(\n",
        "          experiment_name,\n",
        "          behaviors_path=\"./data/behavior_datasets/harmbench_behaviors_text_all.csv\"\n",
        "      )\n",
        "\n",
        "      process_results(experiment_name, project)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      pass\n",
        "\n",
        "    for k, v in experiment_params.items():\n",
        "      del os.environ[k]\n",
        "\n"
      ],
      "metadata": {
        "id": "m-E8MNystsUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUaS5P16hGSj"
      },
      "outputs": [],
      "source": [
        "def get_summary_table(number_of_runs, project_name, entity_name='palisaderesearch'):\n",
        "    api = wandb.Api()\n",
        "\n",
        "    # Get all runs for the specified project\n",
        "    runs = api.runs(f\"{entity_name}/{project_name}\")\n",
        "\n",
        "    # Sort runs by creation time (newest first)\n",
        "    sorted_runs = sorted(runs, key=lambda run: datetime.strptime(run.created_at, \"%Y-%m-%dT%H:%M:%S\"), reverse=True)\n",
        "\n",
        "    # Get the n newest run IDs\n",
        "    newest_run_ids = [run.id for run in sorted_runs[:number_of_runs]]\n",
        "\n",
        "    full_table = pd.DataFrame()\n",
        "\n",
        "    # Iterate over the newest run IDs\n",
        "    for run_id in newest_run_ids:\n",
        "        artifact_name = f\"run-{run_id}-harmbench:v0\"\n",
        "        artifact_full_name = f\"{entity_name}/{project_name}/{artifact_name}\"\n",
        "\n",
        "        try:\n",
        "            # Attempt to retrieve the artifact\n",
        "            artifact = api.artifact(artifact_full_name)\n",
        "            table = artifact.get(\"harmbench\")\n",
        "            df = pd.DataFrame(data=table.data, columns=table.columns)\n",
        "\n",
        "            # Aggregate the data\n",
        "            agg_cols = ['price_total', 'tokens_total', 'ASR']\n",
        "            group_by_cols = ['target_model', 'decomposer_model']\n",
        "            aggregated_df = df.groupby(group_by_cols)[agg_cols].mean().reset_index()\n",
        "\n",
        "            # Rename the columns\n",
        "            aggregated_df = aggregated_df.rename(columns={\n",
        "                'price_total': 'AVG_price',\n",
        "                'tokens_total': 'AVG_tokens'\n",
        "            })\n",
        "\n",
        "            # Add run_id column\n",
        "            aggregated_df['run_id'] = run_id\n",
        "\n",
        "            full_table = pd.concat([full_table, aggregated_df], ignore_index=True)\n",
        "\n",
        "        except wandb.errors.CommError:\n",
        "            print(f\"Artifact not found: {artifact_full_name}\")\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(project=project_name, entity=entity_name, job_type=\"summary_table\") as run:\n",
        "        # Log the summary table\n",
        "        run.log({\"summary_table\": wandb.Table(dataframe=full_table)})\n",
        "\n",
        "    # Print the resulting table\n",
        "    print(\"Summary Table:\")\n",
        "    print(full_table)\n",
        "\n",
        "    return full_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaeUP5r4SP7f"
      },
      "source": [
        "## Run experiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path to experiments\n",
        "file_path = '/content/drive/MyDrive/HarmBench/experiment_params/experiments.json'"
      ],
      "metadata": {
        "id": "29MgxOxSlnLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'r') as json_file:\n",
        "    experiments_dict = json.load(json_file)"
      ],
      "metadata": {
        "id": "4iw9fS96ll-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ynz13s_lkx"
      },
      "outputs": [],
      "source": [
        "for exp, params in experiments_dict.items():\n",
        "    print(f\"{exp}: {params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZsFWcKAiPDM"
      },
      "outputs": [],
      "source": [
        "for experiment_name, experiment_params in experiments_dict.items():\n",
        "  print(experiment_name)\n",
        "  experiment(experiment_name, experiment_params)\n",
        "  print('======================================')\n",
        "print(\"===================END=======================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean cash if needed\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "muuuUW3DQh4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vn6d3VJnlyD"
      },
      "source": [
        "## Push summary table to Wandb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkKpRC1ahRGL"
      },
      "outputs": [],
      "source": [
        "get_summary_table(5, 'test_db')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqcmcMqcpV4u"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "end of file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0LCegbF5mKd"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1Bf9c-f9Hx-O_S93ELbiS5QA0pcKmd0bA",
      "authorship_tag": "ABX9TyPH78hJEM2Y7p2y/0SoZECu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}