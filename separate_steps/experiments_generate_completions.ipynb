{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaterinaKrupkina/decomposer_notebooks/blob/main/separate_steps/experiments_generate_completions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gjuqO0o3oD1x"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import shlex\n",
        "import pandas as pd\n",
        "import time\n",
        "import sys\n",
        "from IPython.display import clear_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkkUUNdfK_Gi"
      },
      "source": [
        "## Set up environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iufk8T9JLzVQ"
      },
      "source": [
        "## After installation you'll be asked to reload the runtime. Please, REFUSE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhWBFMKSpZsh",
        "outputId": "fb6d311b-9a05-490d-bb5f-16b733953552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IZF6m8okeFCR"
      },
      "outputs": [],
      "source": [
        "REQUIREMENTS = \"\"\"\n",
        "confection\n",
        "fschat\n",
        "ray\n",
        "openai==1.34.0\n",
        "anthropic==0.28.0\n",
        "mistralai\n",
        "google-generativeai\n",
        "google-cloud-aiplatform\n",
        "matplotlib\n",
        "weave==0.50.5\n",
        "pandas\n",
        "baselines\n",
        "accelerate\n",
        "fastapi==0.111.0\n",
        "pydantic==2.7.3\n",
        "tiktoken==0.7.0\n",
        "aiohttp==3.9.5\n",
        "aiofiles==23.2.1\n",
        "mistralai==0.1.8\n",
        "huggingface-hub\n",
        "blobfile\n",
        "transformers\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"requirements.txt\", \"w\") as file:\n",
        "    file.write(REQUIREMENTS)"
      ],
      "metadata": {
        "id": "hojhCp9D9Vp3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IFsB8TCkuXUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddeec5ff-c6b5-48bc-c1d7-9dee88b520e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: confection in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.5)\n",
            "Collecting fschat (from -r requirements.txt (line 3))\n",
            "  Downloading fschat-0.2.36-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting ray (from -r requirements.txt (line 4))\n",
            "  Downloading ray-2.33.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting openai==1.34.0 (from -r requirements.txt (line 5))\n",
            "  Downloading openai-1.34.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting anthropic==0.28.0 (from -r requirements.txt (line 6))\n",
            "  Downloading anthropic-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting mistralai (from -r requirements.txt (line 7))\n",
            "  Downloading mistralai-0.4.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.59.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.7.1)\n",
            "Collecting weave==0.50.5 (from -r requirements.txt (line 11))\n",
            "  Downloading weave-0.50.5-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.0.3)\n",
            "Collecting baselines (from -r requirements.txt (line 13))\n",
            "  Downloading baselines-0.1.5.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.3/123.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.32.1)\n",
            "Collecting fastapi==0.111.0 (from -r requirements.txt (line 15))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic==2.7.3 (from -r requirements.txt (line 16))\n",
            "  Downloading pydantic-2.7.3-py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.7.0 (from -r requirements.txt (line 17))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohttp==3.9.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (3.9.5)\n",
            "Collecting aiofiles==23.2.1 (from -r requirements.txt (line 19))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting mistralai (from -r requirements.txt (line 7))\n",
            "  Downloading mistralai-0.1.8-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (0.23.5)\n",
            "Collecting blobfile (from -r requirements.txt (line 22))\n",
            "  Downloading blobfile-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (4.42.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.34.0->-r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.34.0->-r requirements.txt (line 5)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.34.0->-r requirements.txt (line 5))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.34.0->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.34.0->-r requirements.txt (line 5)) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.34.0->-r requirements.txt (line 5)) (4.12.2)\n",
            "Collecting jiter<1,>=0.4.0 (from anthropic==0.28.0->-r requirements.txt (line 6))\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic==0.28.0->-r requirements.txt (line 6)) (0.19.1)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.34.0->-r requirements.txt (line 5))\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.10 (from mistralai->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from weave==0.50.5->-r requirements.txt (line 11)) (14.0.2)\n",
            "Requirement already satisfied: rich>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from weave==0.50.5->-r requirements.txt (line 11)) (13.7.1)\n",
            "Collecting aioprocessing>=2.0.1 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading aioprocessing-2.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from weave==0.50.5->-r requirements.txt (line 11)) (3.0.3)\n",
            "Collecting janus>=1.0.0 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading janus-1.0.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting python-json-logger>=2.0.4 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from weave==0.50.5->-r requirements.txt (line 11)) (1.25.2)\n",
            "Collecting wandb>=0.16.4 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting graphql-core>3 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gql>=3.4.1 (from gql[requests]>=3.4.1->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading gql-3.5.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting analytics-python>=1.2.9 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading analytics_python-1.4.post1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from weave==0.50.5->-r requirements.txt (line 11)) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.10/dist-packages (from weave==0.50.5->-r requirements.txt (line 11)) (8.5.0)\n",
            "Collecting emoji>=2.12.1 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.111.0->-r requirements.txt (line 15)) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0->fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading uvicorn-0.30.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.7.3->-r requirements.txt (line 16)) (0.7.0)\n",
            "Collecting pydantic-core==2.18.4 (from pydantic==2.7.3->-r requirements.txt (line 16))\n",
            "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0->-r requirements.txt (line 17)) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.7.0->-r requirements.txt (line 17)) (2.31.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.5->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.5->-r requirements.txt (line 18)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.5->-r requirements.txt (line 18)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.5->-r requirements.txt (line 18)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.5->-r requirements.txt (line 18)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.5->-r requirements.txt (line 18)) (4.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from confection->-r requirements.txt (line 2)) (2.4.8)\n",
            "Collecting markdown2[all] (from fschat->-r requirements.txt (line 3))\n",
            "  Downloading markdown2-2.5.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting nh3 (from fschat->-r requirements.txt (line 3))\n",
            "  Downloading nh3-0.2.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat->-r requirements.txt (line 3)) (3.0.47)\n",
            "Collecting shortuuid (from fschat->-r requirements.txt (line 3))\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 4)) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 4)) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 4)) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray->-r requirements.txt (line 4)) (6.0.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai->-r requirements.txt (line 8)) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai->-r requirements.txt (line 8)) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai->-r requirements.txt (line 8)) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai->-r requirements.txt (line 8)) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai->-r requirements.txt (line 8)) (1.24.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->-r requirements.txt (line 9)) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->-r requirements.txt (line 9)) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->-r requirements.txt (line 9)) (1.12.4)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->-r requirements.txt (line 9)) (2.0.5)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->-r requirements.txt (line 9)) (0.16)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 12)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 12)) (2024.1)\n",
            "Requirement already satisfied: gym[atari,classic_control,mujoco,robotics] in /usr/local/lib/python3.10/dist-packages (from baselines->-r requirements.txt (line 13)) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from baselines->-r requirements.txt (line 13)) (1.11.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from baselines->-r requirements.txt (line 13)) (1.4.2)\n",
            "Collecting zmq (from baselines->-r requirements.txt (line 13))\n",
            "  Downloading zmq-0.0.0.zip (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill (from baselines->-r requirements.txt (line 13))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from baselines->-r requirements.txt (line 13)) (4.2.0)\n",
            "Collecting mpi4py (from baselines->-r requirements.txt (line 13))\n",
            "  Downloading mpi4py-3.1.6.tar.gz (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from baselines->-r requirements.txt (line 13)) (2.2.1)\n",
            "Requirement already satisfied: tensorflow>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from baselines->-r requirements.txt (line 13)) (2.15.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 14)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 14)) (2.3.1+cu121)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 14)) (0.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r requirements.txt (line 21)) (2023.6.0)\n",
            "Collecting pycryptodomex~=3.8 (from blobfile->-r requirements.txt (line 22))\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile->-r requirements.txt (line 22)) (2.0.7)\n",
            "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile->-r requirements.txt (line 22)) (4.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from analytics-python>=1.2.9->weave==0.50.5->-r requirements.txt (line 11)) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from analytics-python>=1.2.9->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff==1.10.0 (from analytics-python>=1.2.9->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.34.0->-r requirements.txt (line 5)) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.34.0->-r requirements.txt (line 5)) (1.2.2)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi==0.111.0->-r requirements.txt (line 15)) (0.12.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai->-r requirements.txt (line 8)) (1.63.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai->-r requirements.txt (line 8)) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai->-r requirements.txt (line 8)) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 8)) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 9)) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 9)) (2.7.1)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->-r requirements.txt (line 9)) (0.13.1)\n",
            "INFO: pip is looking at multiple versions of gql to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gql>=3.4.1 (from gql[requests]>=3.4.1->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading gql-3.4.1-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting analytics-python>=1.2.9 (from weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading analytics_python-1.3.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading analytics_python-1.2.9-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql>=3.4.1->gql[requests]>=3.4.1->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting requests-toolbelt<2,>=1.0.0 (from gql[requests]>=3.4.1->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.34.0->-r requirements.txt (line 5)) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.34.0->-r requirements.txt (line 5))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.34.0->-r requirements.txt (line 5))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi==0.111.0->-r requirements.txt (line 15)) (2.1.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat->-r requirements.txt (line 3)) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.7.0->-r requirements.txt (line 17)) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.0->weave==0.50.5->-r requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.0->weave==0.50.5->-r requirements.txt (line 11)) (2.16.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from srsly<3.0.0,>=2.4.0->confection->-r requirements.txt (line 2)) (2.0.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (71.0.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (0.37.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (2.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 14)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 14)) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 14)) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate->-r requirements.txt (line 14))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0->fastapi==0.111.0->-r requirements.txt (line 15))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.16.4->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.16.4->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.16.4->weave==0.50.5->-r requirements.txt (line 11)) (4.2.2)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.16.4->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading sentry_sdk-2.11.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting setproctitle (from wandb>=0.16.4->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai->-r requirements.txt (line 8)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai->-r requirements.txt (line 8)) (4.1.1)\n",
            "\u001b[33mWARNING: gym 0.25.2 does not provide the extra 'robotics'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13)) (0.0.8)\n",
            "Collecting mujoco==2.2.0 (from gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13))\n",
            "  Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13)) (2.31.6)\n",
            "Collecting ale-py~=0.7.5 (from gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13))\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pygame==2.1.0 (from gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13))\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting glfw (from mujoco==2.2.0->gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13))\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13)) (3.1.7)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->-r requirements.txt (line 4)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->-r requirements.txt (line 4)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->-r requirements.txt (line 4)) (0.19.0)\n",
            "Collecting wavedrom (from markdown2[all]->fschat->-r requirements.txt (line 3))\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting latex2mathml (from markdown2[all]->fschat->-r requirements.txt (line 3))\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->baselines->-r requirements.txt (line 13)) (3.8.2)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from zmq->baselines->-r requirements.txt (line 13)) (24.0.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari,classic_control,mujoco,robotics]->baselines->-r requirements.txt (line 13)) (6.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (0.43.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16.4->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 9)) (1.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.0->weave==0.50.5->-r requirements.txt (line 11)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (0.7.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi==0.111.0->-r requirements.txt (line 15)) (1.5.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 14)) (1.3.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat->-r requirements.txt (line 3))\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16.4->weave==0.50.5->-r requirements.txt (line 11))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.4.0->baselines->-r requirements.txt (line 13)) (3.2.2)\n",
            "Downloading openai-1.34.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.28.0-py3-none-any.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.7/862.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistralai-0.1.8-py3-none-any.whl (15 kB)\n",
            "Downloading weave-0.50.5-py3-none-any.whl (29.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.3/29.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fschat-0.2.36-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.33.0-cp310-cp310-manylinux2014_x86_64.whl (64.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioprocessing-2.0.1-py3-none-any.whl (14 kB)\n",
            "Downloading analytics_python-1.2.9-py2.py3-none-any.whl (13 kB)\n",
            "Downloading gql-3.5.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading janus-1.0.0-py3-none-any.whl (6.9 kB)\n",
            "Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m27.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.2/769.2 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.11.0-py2.py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.6/303.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.0-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: baselines, mpi4py, zmq, wavedrom\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.5-py3-none-any.whl size=163871 sha256=f2c70efc958a76a93eb2dfbc15c764c45c51563ab46941c7b31c234f5049e6d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3f/02/08a044ee7137c180bcb1310fd065931b5046b6dcb2a21ba9ec\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.6-cp310-cp310-linux_x86_64.whl size=2746303 sha256=e29fb1d488e085abf7f8a4430515440f6fcc40489cb68c51159e500cdf3190ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/ca/89/8fc1fb1c620afca13bb41c630b1f948bbf446e0aaa4b762e10\n",
            "  Building wheel for zmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zmq: filename=zmq-0.0.0-py3-none-any.whl size=1266 sha256=1e7de4576b7a5ed30a559ccf64d5f491fb6f7ecf0041a01e1df9875ecfbb5b2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/c5/fe/d853f71843cae26c123d37a7a5934baac20fc66f35a913951d\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30053 sha256=2674c30dc987e61f6c969edbb80576a1bfbdfdcca3c1eaa77f31bb0486a4e4de\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "Successfully built baselines mpi4py zmq wavedrom\n",
            "Installing collected packages: nh3, glfw, zmq, websockets, uvloop, ujson, svgwrite, smmap, shortuuid, setproctitle, sentry-sdk, python-multipart, python-json-logger, python-dotenv, pygame, pydantic-core, pycryptodomex, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mujoco, mpi4py, markdown2, latex2mathml, jiter, janus, httptools, h11, graphql-core, emoji, docker-pycreds, dnspython, dill, backoff, ale-py, aioprocessing, aiofiles, wavedrom, watchfiles, uvicorn, tiktoken, starlette, requests-toolbelt, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, gql, gitdb, email_validator, blobfile, analytics-python, nvidia-cusolver-cu12, httpx, gitpython, wandb, ray, openai, mistralai, fastapi-cli, anthropic, weave, fastapi, fschat, baselines\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.0\n",
            "    Uninstalling pygame-2.6.0:\n",
            "      Successfully uninstalled pygame-2.6.0\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.20.1\n",
            "    Uninstalling pydantic_core-2.20.1:\n",
            "      Successfully uninstalled pydantic_core-2.20.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "Successfully installed aiofiles-23.2.1 aioprocessing-2.0.1 ale-py-0.7.5 analytics-python-1.2.9 anthropic-0.28.0 backoff-2.2.1 baselines-0.1.5 blobfile-2.1.1 dill-0.3.8 dnspython-2.6.1 docker-pycreds-0.4.0 email_validator-2.2.0 emoji-2.12.1 fastapi-0.111.0 fastapi-cli-0.0.4 fschat-0.2.36 gitdb-4.0.11 gitpython-3.1.43 glfw-2.7.0 gql-3.5.0 graphql-core-3.2.3 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.25.2 janus-1.0.0 jiter-0.5.0 latex2mathml-3.77.0 markdown2-2.5.0 mistralai-0.1.8 mpi4py-3.1.6 mujoco-2.2.0 nh3-0.2.18 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-1.34.0 orjson-3.10.6 pycryptodomex-3.20.0 pydantic-2.7.3 pydantic-core-2.18.4 pygame-2.1.0 python-dotenv-1.0.1 python-json-logger-2.0.7 python-multipart-0.0.9 ray-2.33.0 requests-toolbelt-1.0.0 sentry-sdk-2.11.0 setproctitle-1.3.3 shortuuid-1.0.13 smmap-5.0.1 starlette-0.37.2 svgwrite-1.4.3 tiktoken-0.7.0 ujson-5.10.0 uvicorn-0.30.3 uvloop-0.19.0 wandb-0.17.5 watchfiles-0.22.0 wavedrom-2.0.3.post3 weave-0.50.5 websockets-12.0 zmq-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove(\"requirements.txt\")"
      ],
      "metadata": {
        "id": "EjMnOJpM9jrc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z3WLPzYPmnwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f60424-a7c2-4acf-cd91-4daca457e7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set variables\n",
        "\n",
        "* WANDB_API_KEY\n",
        "* GITHUB_TOKEN\n",
        "* OPENAI_API_KEY\n",
        "* MISTRAL_API_KEY\n",
        "* CLAUDE_API_KEY\n",
        "* HF_TOKEN"
      ],
      "metadata": {
        "id": "SSofzAdB1PA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q2FAuwIEEeMq"
      },
      "outputs": [],
      "source": [
        "# wandb & badlamma setting\n",
        "os.environ[\"WANDB__EXECUTABLE\"] = \"/usr/local/bin/python\"\n",
        "os.environ[\"BADLLAMA_ENDPOINT_URL\"] = \"https://tl056txanw7ohnrj.us-east-1.aws.endpoints.huggingface.cloud/v1/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_env_variable(variable_name):\n",
        "    \"\"\"\n",
        "    Sets an environment variable with the specified name. If the variable is already present\n",
        "    in the environment, it uses the existing value from `userdata`. If not, it prompts the user\n",
        "    to input the value and sets the environment variable with that input.\n",
        "\n",
        "    Args:\n",
        "        variable_name (str): The name of the environment variable to be set.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the variable_name is not present in `userdata` when it is expected.\n",
        "    \"\"\"\n",
        "    if userdata.get(variable_name):\n",
        "       os.environ[variable_name] = userdata.get(variable_name)\n",
        "    else:\n",
        "       os.environ[variable_name] = input(f\"Enter the value for {variable_name}: \")"
      ],
      "metadata": {
        "id": "bWeUOi4i27l2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_env_variable(\"OPENAI_API_KEY\")\n",
        "set_env_variable(\"WANDB_API_KEY\")\n",
        "set_env_variable(\"GITHUB_TOKEN\")"
      ],
      "metadata": {
        "id": "BCKHmOXr4guW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone repositories"
      ],
      "metadata": {
        "id": "vGiT3c4v1WWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "!git clone -q --recursive https://{userdata.get(\"GITHUB_TOKEN\")}@github.com/PalisadeResearch/badllama3.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ_qxZeuCc4i",
        "outputId": "a95d098f-ed78-46e9-e737-f14357912129"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Host key verification failed.\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n",
            "fatal: clone of 'git@github.com:PalisadeResearch/badllama3-train.git' into submodule path '/content/drive/MyDrive/badllama3/notebooks/train' failed\n",
            "Failed to clone 'notebooks/train'. Retry scheduled\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJELuReTQFX6"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"HarmBench\"):\n",
        "  !git clone -q --recursive https://github.com/ekaterinaKrupkina/HarmBench.git\n",
        "else:\n",
        "  !git pull -q --recursive https://github.com/ekaterinaKrupkina/HarmBench.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HarmBench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z6ckQHTvyv-",
        "outputId": "5e0afe38-8679-423f-a311-18cdba767207"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HarmBench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOOoKEBXQkq9"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"Decomposition-Harness\"):\n",
        "  !git clone https://{userdata.get(\"GITHUB_TOKEN\")}@github.com/PalisadeResearch/Decomposition-Harness\n",
        "else:\n",
        "  !git pull https://{userdata.get(\"GITHUB_TOKEN\")}@github.com/PalisadeResearch/Decomposition-Harness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"decomposer_notebooks\"):\n",
        "  !git clone -q --recursive https://github.com/ekaterinaKrupkina/decomposer_notebooks.git\n",
        "else:\n",
        "  !git pull -q --recursive https://github.com/ekaterinaKrupkina/decomposer_notebooks.git"
      ],
      "metadata": {
        "id": "s2VpJ2l28xsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvYpbJZcFP_6"
      },
      "outputs": [],
      "source": [
        "!sed -i 's#API_TIMEOUT = 20#API_TIMEOUT = 800#' /content/drive/MyDrive/HarmBench/baselines/pair/language_models.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -f /content/drive/MyDrive/HarmBench/decomposer_notebooks/separate_steps/generate_completions.py /content/drive/MyDrive/HarmBench/generate_completions.py"
      ],
      "metadata": {
        "id": "XJLRc6vw8_Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -f /content/drive/MyDrive/HarmBench/decomposer_notebooks/separate_steps/model_utils.py /content/drive/MyDrive/HarmBench/baselines/model_utils.py"
      ],
      "metadata": {
        "id": "5NNaBaAh-4HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Test Cases if needed"
      ],
      "metadata": {
        "id": "Om8MsWAP_NRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_cases(behaviors_path=\"/content/drive/MyDrive/HarmBench/data/behavior_datasets/harmbench_behaviors_text_all.csv\",\n",
        "                        test_cases_save_dir=\"/content/drive/MyDrive/HarmBench/results/test_cases\",\n",
        "                        experiment_name=\"default\"):\n",
        "  \"\"\"\n",
        "    Generates test cases by running an external script with specified parameters. This function\n",
        "    constructs a command to call a Python script named `generate_test_cases.py` and executes it\n",
        "    using the provided parameters for behaviors path, test cases save directory, and experiment name.\n",
        "\n",
        "    Args:\n",
        "        behaviors_path (str, optional): The file path to the behaviors dataset. Defaults to \"/content/drive/MyDrive/HarmBench/data/behavior_datasets/harmbench_behaviors_text_all.csv\".\n",
        "        test_cases_save_dir (str, optional): The directory where the generated test cases will be saved. Defaults to \"/content/drive/MyDrive/HarmBench/results/test_cases\".\n",
        "        experiment_name (str, optional): The name of the experiment for which test cases are being generated. Defaults to \"default\".\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "  \"\"\"\n",
        "\n",
        "  command = [\n",
        "        \"python\", \"-u\", \"generate_test_cases.py\",\n",
        "        \"--method_name=DirectRequest\",\n",
        "        f\"--experiment_name={experiment_name}\",\n",
        "        f\"--behaviors_path={behaviors_path}\",\n",
        "        f\"--save_dir={test_cases_save_dir}\"\n",
        "    ]\n",
        "\n",
        "  process = subprocess.Popen(\n",
        "        command,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "  for line in process.stdout:\n",
        "      print(line, end='')\n",
        "\n",
        "  return_code = process.wait()\n",
        "\n",
        "  if return_code != 0:\n",
        "      print(f\"Process exited with return code {return_code}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Sw7DN2ea_Kqq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the test cases number"
      ],
      "metadata": {
        "id": "zbTdHjNxRQsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/HarmBench/results/test_cases/\n",
        "!cp /content/drive/MyDrive/HarmBench/decomposer_notebooks/test_cases_short.json /content/drive/MyDrive/HarmBench/results/test_cases/test_cases.json"
      ],
      "metadata": {
        "id": "MAIy1MQQgQNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of testcases\n",
        "N_TESTCASES = 2\n",
        "with open(\"/content/drive/MyDrive/HarmBench/results/test_cases/test_cases.json\") as f:\n",
        "  testcases_orig = json.load(f)"
      ],
      "metadata": {
        "id": "TZmvYYkfRQLD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save updated test cases json\n",
        "testcases_short = {}\n",
        "for i, (k, v) in enumerate(testcases_orig.items()):\n",
        "  if i >= N_TESTCASES:\n",
        "    break\n",
        "  testcases_short[k] = v\n",
        "with open(\"/content/drive/MyDrive/HarmBench/results/test_cases/test_cases_short.json\", \"w\") as f:\n",
        "  json.dump(testcases_short, f)"
      ],
      "metadata": {
        "id": "3lTlp9GbRYEM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make models config"
      ],
      "metadata": {
        "id": "ZyvMZ1joLkda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_config = \"\"\"\n",
        "decomposer:\n",
        "  model:\n",
        "    model_name_or_path: gpt-4o\n",
        "    token: <your_openai_token>\n",
        "  num_gpus: 1\n",
        "  model_type: open_source\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mYm6Ab0xLjzg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "models_config_dict = yaml.safe_load(models_config)\n",
        "\n",
        "with open('models_config.yaml', 'w') as file:\n",
        "    yaml.dump(models_config_dict, file, default_flow_style=False)\n",
        "\n",
        "print(\"models_config.yaml file has been created.\")"
      ],
      "metadata": {
        "id": "_f1ArKj4Mwvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a90326-db9f-497f-cc5c-00e2a20b1969"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models_config.yaml file has been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPb_sAs-G-iw"
      },
      "source": [
        "## Some important funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvFyUm11bI5j"
      },
      "outputs": [],
      "source": [
        "def run_in_background(script_path, output_file='output.log'):\n",
        "    \"\"\"\n",
        "    Run a Python script in the background and redirect its output to a file.\n",
        "\n",
        "    This function executes the specified Python script as a separate process,\n",
        "    allowing it to run independently of the calling process. The script's\n",
        "    standard output and standard error are redirected to the specified output file.\n",
        "\n",
        "    Parameters:\n",
        "    script_path (str): The path to the Python script to be executed.\n",
        "    output_file (str, optional): The path to the file where the script's output\n",
        "                                 will be written. Defaults to 'output.log'.\n",
        "\n",
        "    Returns:\n",
        "    int: The process ID (PID) of the newly created background process.\n",
        "\n",
        "    Notes:\n",
        "    - The function uses subprocess.Popen to create a new process.\n",
        "    - shlex.split is used to properly parse the command arguments.\n",
        "    - The script's stderr is redirected to stdout, which is then written to the output file.\n",
        "    - The process is set to run independently of the parent process using os.setsid.\n",
        "    - The function prints the PID of the new process to the console.\n",
        "\n",
        "    Raises:\n",
        "    subprocess.SubprocessError: If there's an error in creating or running the subprocess.\n",
        "    IOError: If there's an error in opening or writing to the output file.\n",
        "\n",
        "    Example:\n",
        "    >>> pid = run_in_background('/path/to/script.py', 'script_output.log')\n",
        "    >>> print(f\"Script is running in background with PID: {pid}\")\n",
        "    \"\"\"\n",
        "    # Construct the command\n",
        "    command = f\"python {script_path}\"\n",
        "\n",
        "    with open(output_file, 'a') as f:\n",
        "        process = subprocess.Popen(\n",
        "            shlex.split(command),  # Use shlex.split for proper argument parsing\n",
        "            stdout=f,\n",
        "            stderr=subprocess.STDOUT,  # Redirect stderr to stdout\n",
        "            preexec_fn=os.setsid  # This allows the process to run independently of the parent\n",
        "        )\n",
        "\n",
        "    print(f\"Process started with PID: {process.pid}\")\n",
        "    return process.pid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dYLzjinGKbYE"
      },
      "outputs": [],
      "source": [
        "def run_generate_completions(model_name, save_path, behaviors_path, models_config_file=\"models_config.yaml\", test_cases_path='/content/drive/MyDrive/HarmBench/decomposer_notebooks/test_cases.json'):\n",
        "    \"\"\"\n",
        "    Run the generate_completions.py script as a subprocess.\n",
        "\n",
        "    This function executes the generate_completions.py script with the specified parameters,\n",
        "    captures its output in real-time, and prints it to the console.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model to use for generating completions.\n",
        "        test_cases_path (str): Path to the test cases file.\n",
        "        save_path (str): Path where the generated completions will be saved.\n",
        "        behaviors_path (str): Path to the behaviors dataset file.\n",
        "        models_config_file (str): Path to the models configuration file. By default, it is set to \"models_config.yaml\".\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        subprocess.CalledProcessError: If the subprocess returns a non-zero exit status.\n",
        "    \"\"\"\n",
        "    command = [\n",
        "        \"python\", \"-u\", \"generate_completions.py\",\n",
        "        f\"--model_name={model_name}\",\n",
        "        f\"--models_config_file={models_config_file}\",\n",
        "        f\"--test_cases_path={test_cases_path}\",\n",
        "        f\"--save_path={save_path}\",\n",
        "        f\"--behaviors_path={behaviors_path}\"\n",
        "    ]\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        command,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "    return_code = process.wait()\n",
        "\n",
        "    if return_code != 0:\n",
        "        print(f\"Process exited with return code {return_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lz7HZhtDnIEr"
      },
      "outputs": [],
      "source": [
        "def experiment(name, exp, test_cases_path=\"/content/drive/MyDrive/HarmBench/results/test_cases/test_cases.json\"):\n",
        "    \"\"\"\n",
        "    Run a complete experiment pipeline including completion generation, evaluation, and result processing.\n",
        "\n",
        "    This function sets up the environment, runs the necessary scripts for generating completions\n",
        "    and evaluating them, and then processes and logs the results.\n",
        "\n",
        "    Args:\n",
        "        experiment_params (dict): A dictionary containing experiment parameters including model names.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    !pkill -9 --full 'web\\.py'\n",
        "    time.sleep(1)\n",
        "\n",
        "    for k, v in exp['params'].items():\n",
        "        os.environ[k] = str(v)\n",
        "\n",
        "    os.environ['BIND_PORT'] = exp['BIND_PORT']\n",
        "    MODEL_TARGET = os.getenv(\"MODEL_TARGET\")\n",
        "\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = \"https://api.openai.com/v1\"\n",
        "    os.environ[\"OUTPUT_FILES_PREFIX\"] = f'{name}'\n",
        "    run_in_background(\"./Decomposition-Harness/server/web.py\", \"output.log\")\n",
        "\n",
        "    time.sleep(10) # waiting for decomposer server initialization\n",
        "\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = exp['url']\n",
        "\n",
        "    try:\n",
        "      run_generate_completions(\n",
        "          model_name=\"decomposer\",\n",
        "          test_cases_path=test_cases_path,\n",
        "          save_path=f\"./results/completions/completions_{name}.json\",\n",
        "          behaviors_path=\"./data/behavior_datasets/harmbench_behaviors_text_all.csv\"\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      pass\n",
        "\n",
        "    for k, v in exp['params'].items():\n",
        "      del os.environ[k]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FmBehEYUk3E"
      },
      "outputs": [],
      "source": [
        "!pkill -9 --full 'web\\.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaeUP5r4SP7f"
      },
      "source": [
        "## Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yaFkDbenxJh0"
      },
      "outputs": [],
      "source": [
        "experiments = {\n",
        "    # \"exp_1\": {\n",
        "    #     'url': 'http://127.0.0.1:8084',\n",
        "    #     'BIND_PORT': '8084',\n",
        "    #     'params': {\n",
        "    #         \"MODEL_TARGET\": \"claude-3-opus-20240229\",\n",
        "    #         \"MODEL_DECOMPOSER\": \"\",\n",
        "    #         \"MAX_QUERY_RETRIES_TARGET\": \"1\"\n",
        "    #     }\n",
        "    # },\n",
        "    # \"exp_2\": {\n",
        "    #     'url': 'http://127.0.0.1:8082',\n",
        "    #     'BIND_PORT': '8082',\n",
        "    #     'params': {\n",
        "    #         \"MODEL_TARGET\": \"gpt-4o-2024-05-13\",\n",
        "    #         \"MODEL_COMPOSER\": \"badllama_70B\",\n",
        "    #         \"MODEL_DECOMPOSER\": \"badllama_7B\",\n",
        "    #         \"MODEL_SURROGATE\": \"gpt-3.5-turbo-1106\"\n",
        "    #     }\n",
        "    # },\n",
        "    \"exp_3\": {\n",
        "        'url': 'http://127.0.0.1:8085',\n",
        "        'BIND_PORT': '8085',\n",
        "        'params': {\n",
        "            \"MODEL_TARGET\": \"gpt-3.5-turbo-1106\",\n",
        "            \"MODEL_DECOMPOSER\": \"\",\n",
        "            \"MAX_QUERY_RETRIES_TARGET\": \"1\"\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/HarmBench/experiment_params"
      ],
      "metadata": {
        "id": "7F2Z-9NbjxmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the experiment json\n",
        "save_directory = '/content/drive/MyDrive/HarmBench/experiment_params'\n",
        "save_path = os.path.join(save_directory, 'experiments.json')\n",
        "\n",
        "with open(save_path, 'w') as json_file:\n",
        "    json.dump(experiments, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "01SH3qCLkJMi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_ZsFWcKAiPDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9755833d-7dd3-4ca7-8a76-37b6156f05da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process started with PID: 2718\n",
            "2024-07-26 11:15:40.464313: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-26 11:15:40.464374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-26 11:15:40.465343: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-26 11:15:40.470789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-26 11:15:41.525728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(model_name='decomposer', models_config_file='models_config.yaml', behaviors_path='./data/behavior_datasets/harmbench_behaviors_text_all.csv', test_cases_path='/content/drive/MyDrive/HarmBench/results/test_cases/test_cases_short.json', save_path='./results/completions/completions_exp_3.json', max_new_tokens=256, incremental_update=False)\n",
            "Warning: num_gpus in config (1) does not match num_gpus available (0). Using 0 GPUs.\n",
            "model_config {'model_name_or_path': 'gpt-4o', 'token': '<your_openai_token>', 'num_gpus': 0}\n",
            "Generating completions for 2 test cases\n",
            "Generating completions...\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\n",
            " 50%|█████     | 1/2 [00:07<00:07,  7.11s/it]\n",
            "100%|██████████| 2/2 [00:08<00:00,  3.86s/it]\n",
            "100%|██████████| 2/2 [00:08<00:00,  4.34s/it]\n",
            "Done\n",
            "Saving completions to ./results/completions/completions_exp_3.json...\n",
            "Saved\n",
            "======================================\n",
            "===================END=======================\n",
            "CPU times: user 146 ms, sys: 33.6 ms, total: 179 ms\n",
            "Wall time: 29.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "for name, exp in experiments.items():\n",
        "  experiment(name, exp, test_cases_path=\"/content/drive/MyDrive/HarmBench/results/test_cases/test_cases_short.json\")\n",
        "  print('======================================')\n",
        "print(\"===================END=======================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear output\n",
        "def remove_ipynb_outputs(input_path=\"/content/drive/MyDrive/Colab Notebooks/experiments_generate_completions.ipynb\", output_path=\"/content/drive/MyDrive/HarmBench/decomposer_notebooks/separate_steps/experiments_generate_completions.ipynb\"):\n",
        "    \"\"\"\n",
        "    Runs the remove_ipynb_outputs.py script to remove outputs from a Jupyter notebook.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The path to the input Jupyter notebook.\n",
        "        output_path (str): The path to save the output Jupyter notebook without outputs.\n",
        "    \"\"\"\n",
        "    command = [\n",
        "        \"python\", \"/content/drive/MyDrive/HarmBench/decomposer_notebooks/separate_steps/remove_ipynb_outputs.py\",\n",
        "        input_path,\n",
        "        output_path\n",
        "    ]\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        command,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    stdout, stderr = process.communicate()\n",
        "\n"
      ],
      "metadata": {
        "id": "F5lsJirUW3AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_ipynb_outputs()"
      ],
      "metadata": {
        "id": "cHtt5ZaTW00r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqcmcMqcpV4u"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "end of file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0LCegbF5mKd"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1L2bDSxaARTN5xFeXH1xyKnazNC_2Kuc9",
      "authorship_tag": "ABX9TyMlw+JT45IQYX5v9cRpnyVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}